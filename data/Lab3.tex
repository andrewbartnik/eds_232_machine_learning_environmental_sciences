% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Lab 3},
  pdfauthor={Andrew Bartnik},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Lab 3}
\author{Andrew Bartnik}
\date{2023-01-25}

\begin{document}
\maketitle

\hypertarget{lab-3-predicting-the-age-of-abalone}{%
\subsection{Lab 3: Predicting the age of
abalone}\label{lab-3-predicting-the-age-of-abalone}}

Abalones are marine snails. Their flesh is widely considered to be a
desirable food, and is consumed raw or cooked by a variety of cultures.
The age of abalone is determined by cutting the shell through the cone,
staining it, and counting the number of rings through a microscope -- a
boring and time-consuming task. Other measurements, which are easier to
obtain, are used to predict the age.

The data set provided includes variables related to the sex, physical
dimensions of the shell, and various weight measurements, along with the
number of rings in the shell. Number of rings is the stand-in here for
age.

\hypertarget{data-exploration}{%
\subsubsection{Data Exploration}\label{data-exploration}}

Pull the abalone data from Github and take a look at it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{abdat}\OtherTok{\textless{}{-}}\NormalTok{ dat }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"https://raw.githubusercontent.com/MaRo406/eds{-}232{-}machine{-}learning/main/data/abalone{-}data.csv"}\NormalTok{)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## New names:
## Rows: 4177 Columns: 10
## -- Column specification
## -------------------------------------------------------- Delimiter: "," chr
## (1): Sex dbl (9): ...1, Length, Diameter, Height, Whole_weight, Shucked_weight,
## Visce...
## i Use `spec()` to retrieve the full column specification for this data. i
## Specify the column types or set `show_col_types = FALSE` to quiet this message.
## * `` -> `...1`
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(abdat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 4,177
## Columns: 9
## $ Sex            <chr> "M", "M", "F", "M", "I", "I", "F", "F", "M", "F", "F", ~
## $ Length         <dbl> 0.455, 0.350, 0.530, 0.440, 0.330, 0.425, 0.530, 0.545,~
## $ Diameter       <dbl> 0.365, 0.265, 0.420, 0.365, 0.255, 0.300, 0.415, 0.425,~
## $ Height         <dbl> 0.095, 0.090, 0.135, 0.125, 0.080, 0.095, 0.150, 0.125,~
## $ Whole_weight   <dbl> 0.5140, 0.2255, 0.6770, 0.5160, 0.2050, 0.3515, 0.7775,~
## $ Shucked_weight <dbl> 0.2245, 0.0995, 0.2565, 0.2155, 0.0895, 0.1410, 0.2370,~
## $ Viscera_weight <dbl> 0.1010, 0.0485, 0.1415, 0.1140, 0.0395, 0.0775, 0.1415,~
## $ Shell_weight   <dbl> 0.150, 0.070, 0.210, 0.155, 0.055, 0.120, 0.330, 0.260,~
## $ Rings          <dbl> 15, 7, 9, 10, 7, 8, 20, 16, 9, 19, 14, 10, 11, 10, 10, ~
\end{verbatim}

\textbf{I'm cutting out the first column here - it contains the index of
each row and I don't want it to interfere with our predictions.}

\hypertarget{data-splitting}{%
\subsubsection{Data Splitting}\label{data-splitting}}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 1}}. Split the data into training and test
  sets. Use a 70/30 training/test split.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#split the data into training and testing}
\NormalTok{split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(abdat, }\AttributeTok{prop =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ Rings)}
\NormalTok{ab\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(split)}
\NormalTok{ab\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(split)}

\CommentTok{\#taking a look at the training data}
\NormalTok{skimr}\SpecialCharTok{::}\FunctionTok{skim}\NormalTok{(ab\_train)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}ll@{}}
\caption{Data summary}\tabularnewline
\toprule()
\endhead
Name & ab\_train \\
Number of rows & 2922 \\
Number of columns & 9 \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ & \\
Column type frequency: & \\
character & 1 \\
numeric & 8 \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ & \\
Group variables & None \\
\bottomrule()
\end{longtable}

\textbf{Variable type: character}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1944}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1389}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1944}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0556}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0556}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1528}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
skim\_variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_missing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
complete\_rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
min
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
max
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
empty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_unique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
whitespace
\end{minipage} \\
\midrule()
\endhead
Sex & 0 & 1 & 1 & 1 & 0 & 3 & 0 \\
\bottomrule()
\end{longtable}

\textbf{Variable type: numeric}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1829}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1220}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1707}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0610}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0610}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0610}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0610}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0610}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0732}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0732}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0732}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
skim\_variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_missing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
complete\_rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mean
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sd
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p25
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p50
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p75
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
hist
\end{minipage} \\
\midrule()
\endhead
Length & 0 & 1 & 0.52 & 0.12 & 0.07 & 0.45 & 0.54 & 0.62 & 0.78 &
▁▂▅▇▃ \\
Diameter & 0 & 1 & 0.41 & 0.10 & 0.06 & 0.35 & 0.42 & 0.48 & 0.63 &
▁▂▅▇▂ \\
Height & 0 & 1 & 0.14 & 0.04 & 0.00 & 0.12 & 0.14 & 0.16 & 0.24 &
▁▂▆▇▂ \\
Whole\_weight & 0 & 1 & 0.83 & 0.49 & 0.00 & 0.44 & 0.80 & 1.15 & 2.83 &
▇▇▅▁▁ \\
Shucked\_weight & 0 & 1 & 0.36 & 0.22 & 0.00 & 0.19 & 0.34 & 0.50 & 1.49
& ▇▇▂▁▁ \\
Viscera\_weight & 0 & 1 & 0.18 & 0.11 & 0.00 & 0.09 & 0.17 & 0.25 & 0.58
& ▇▇▅▂▁ \\
Shell\_weight & 0 & 1 & 0.24 & 0.14 & 0.00 & 0.13 & 0.23 & 0.33 & 0.90 &
▇▇▃▁▁ \\
Rings & 0 & 1 & 9.93 & 3.20 & 1.00 & 8.00 & 9.00 & 11.00 & 29.00 &
▁▇▂▁▁ \\
\bottomrule()
\end{longtable}

We'll follow our text book's lead and use the caret package in our
approach to this task. We will use the glmnet package in order to
perform ridge regression and the lasso. The main function in this
package is glmnet(), which can be used to fit ridge regression models,
lasso models, and more. In particular, we must pass in an x matrix of
predictors as well as a y outcome vector , and we do not use the y∼x
syntax.

\hypertarget{fit-a-ridge-regression-model}{%
\subsubsection{Fit a ridge regression
model}\label{fit-a-ridge-regression-model}}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 2}}. Use the model.matrix() function to create
  a predictor matrix, x, and assign the Rings variable to an outcome
  vector, y.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Using model.matrix to create a predictor matrix}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(Rings }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., ab\_train)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\CommentTok{\#assigning rings variable to y}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ ab\_train}\SpecialCharTok{$}\NormalTok{Rings}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 3}}. Fit a ridge model (controlled by the alpha
  parameter) using the glmnet() function. Make a plot showing how the
  estimated coefficients change with lambda. (Hint: You can call plot()
  directly on the glmnet() objects).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fitting and plotting a ridge model}
\NormalTok{ridge }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(ridge, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Coefficient magnitude as penalization increases}\SpecialCharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Lab3_files/figure-latex/unnamed-chunk-3-1.pdf}

\hypertarget{using-k-fold-cross-validation-resampling-and-tuning-our-models}{%
\subsubsection{\texorpdfstring{Using \emph{k}-fold cross validation
resampling and tuning our
models}{Using k-fold cross validation resampling and tuning our models}}\label{using-k-fold-cross-validation-resampling-and-tuning-our-models}}

In lecture we learned about two methods of estimating our model's
generalization error by resampling, cross validation and bootstrapping.
We'll use the \emph{k}-fold cross validation method in this lab. Recall
that lambda is a tuning parameter that helps keep our model from
over-fitting to the training data. Tuning is the process of finding the
optima value of lamba.

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 4}}. This time fit a ridge regression model and
  a lasso model, both with using cross validation. The glmnet package
  kindly provides a cv.glmnet() function to do this (similar to the
  glmnet() function that we just used). Use the alpha argument to
  control which type of model you are running. Plot the results.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fitting both ridge and lasso model with 10{-}fold CV}
\NormalTok{ridge2 }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{) }
\NormalTok{lasso2 }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{) }

\CommentTok{\#plotting them}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ridge2, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Ridge penalty}\SpecialCharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(lasso2, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Lasso penalty}\SpecialCharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Lab3_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{itemize}
\item
  \textbf{\emph{Question 5}}. Interpret the graphs. What is being show
  on the axes here? How does the performance of the models change with
  the value of lambda?

  \textbf{The y axis represents the Mean Squared Error (MSE), and
  appears to be its lowest at the smallest x-value (our penalty, logλ),
  suggesting that an OLS model will be a good choice. The number of
  predictors are shown in the upper x-axis. In general, as we increase
  our penalty we constrain both of our models. However, as we go farther
  along the x axis, the MSE increases at a much higher rate in the ridge
  model than the lasso model - suggesting that the lasso model will
  perform better than the ridge model while also selecting for
  predictors.}
\item
  \textbf{\emph{Question 6}}. Inspect the ridge model object you created
  with cv.glmnet(). The \$cvm column shows the MSEs for each cv fold.
  What is the minimum MSE? What is the value of lambda associated with
  this MSE minimum?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#finding the minimum MSE}
\FunctionTok{min}\NormalTok{(ridge2}\SpecialCharTok{$}\NormalTok{cvm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.978248
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# and its lambda value}
\NormalTok{ridge2}\SpecialCharTok{$}\NormalTok{lambda.min}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2012142
\end{verbatim}

\textbf{The smallest MSE for the ridge model is 4.97, and its associated
lambda is 0.2012.}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 7}}. Do the same for the lasso model. What is
  the minimum MSE? What is the value of lambda associated with this MSE
  minimum?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Finding the minimum MSE}
\FunctionTok{min}\NormalTok{(lasso2}\SpecialCharTok{$}\NormalTok{cvm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.652762
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# and its lambda value}
\NormalTok{lasso2}\SpecialCharTok{$}\NormalTok{lambda.min}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0006743901
\end{verbatim}

\textbf{The smallest MSE for the lasso model is 4.65, and its associated
lambda is 0.00067.}

Data scientists often use the ``one-standard-error'' rule when tuning
lambda to select the best model. This rule tells us to pick the most
parsimonious model (fewest number of predictors) while still remaining
within one standard error of the overall minimum cross validation error.
The cv.glmnet() model object has a column that automatically finds the
value of lambda associated with the model that produces an MSE that is
one standard error from the MSE minimum (\$lambda.1se).

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Question 8.}} Find the number of predictors associated
  with this model (hint: the \$nzero is the \# of predictors column).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Finding number of predictors associated with the optimal model {-} ridge}
\NormalTok{ridge2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:  cv.glmnet(x = X, y = y, alpha = 0) 
## 
## Measure: Mean-Squared Error 
## 
##     Lambda Index Measure     SE Nonzero
## min 0.2012   100   4.978 0.2116       9
## 1se 0.3516    94   5.168 0.2279       9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#finding lambda within 1 se of lowest MSE value}
\NormalTok{ridge2}\SpecialCharTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3516275
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#number of predictors associated with this best lambda value}
\NormalTok{ridge2}\SpecialCharTok{$}\NormalTok{nzero[}\DecValTok{94}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## s93 
##   9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Finding number of predictors associated with the optimal model {-} lasso}
\NormalTok{lasso2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:  cv.glmnet(x = X, y = y, alpha = 1) 
## 
## Measure: Mean-Squared Error 
## 
##      Lambda Index Measure     SE Nonzero
## min 0.00067    87   4.653 0.2587       9
## 1se 0.05866    39   4.899 0.3162       5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#finding lambda within 1 se of lowest MSE value}
\NormalTok{lasso2}\SpecialCharTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05865501
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#number of predictors associated with this best lambda value}
\NormalTok{lasso2}\SpecialCharTok{$}\NormalTok{nzero[}\DecValTok{39}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## s38 
##   5
\end{verbatim}

\begin{verbatim}
-   **There are 9 predictors associated with the optimal ridge model, and 5 predictors associated with the optimal lasso model.**
\end{verbatim}

\begin{itemize}
\item
  \textbf{Question 9.} Which regularized regression worked better for
  this task, ridge or lasso? Explain your answer.

  \begin{itemize}
  \tightlist
  \item
    \textbf{The MSE for the best lasso model within 1 standard error of
    the MSE minimum was 4.899, while the MSE for the best ridge model
    within 1 standard error of the MSE minimum was 5.168, indicating
    that the lasso model outperforms the ridge model. Lasso also made
    the model more parsimonious - it was able to reduce the number of
    features necessary for prediction from 9 to 5 while remaining within
    1 standard error of the minimum cross validation error.}
  \end{itemize}
\end{itemize}

\end{document}
